{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question1\n",
    "def load_data(filepath):\n",
    "  df = pd.read_csv(filepath)\n",
    "  df.head()\n",
    "  Y = df['5'].to_numpy()\n",
    "  del df['5']\n",
    "  X = df.to_numpy()\n",
    "  return X, Y\n",
    "\n",
    "X, y = load_data(\"mnist_train.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                X, y, test_size=0.3, random_state=42)\n",
    "#One hot encoding of training labels \n",
    "Labels=pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2\n",
    "class Perceptron():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "\n",
    "\n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            x=self.softmax(z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            prev_term=self.delta_mll(y,self.y_pred)  \n",
    "            # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "            self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            x=self.softmax(z) \n",
    "        return np.argmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0015373832867067966\n",
      "1 0.0014641601415438855\n",
      "2 0.0013973295334736115\n",
      "3 0.0011700113684011207\n",
      "4 0.0011865572495212452\n",
      "5 0.0012489353875638411\n",
      "6 0.0011753742870572596\n",
      "7 0.0010465236922255399\n",
      "8 0.0010842991486821076\n",
      "9 0.0009344390620956329\n",
      "10 0.0007138611606145924\n",
      "11 0.0005668255123983662\n",
      "12 0.0003678119470801297\n",
      "13 0.0002808337801169118\n",
      "14 0.000250976357750367\n",
      "15 0.0002026352864272732\n",
      "16 0.00016389797685827814\n",
      "17 0.00013668304620253644\n",
      "18 0.00010793064852517565\n",
      "19 9.84301600801705e-05\n",
      "20 8.950101116080666e-05\n",
      "21 7.75720135575167e-05\n",
      "22 7.084946772819047e-05\n",
      "23 6.525940232625721e-05\n",
      "24 6.143230120543388e-05\n",
      "25 5.746171615042399e-05\n",
      "26 5.600674684217869e-05\n",
      "27 5.670239326887923e-05\n",
      "28 5.882151232259818e-05\n",
      "29 6.11191994956797e-05\n"
     ]
    }
   ],
   "source": [
    "n=Perceptron(X_train,Labels)\n",
    "n.connect(X_train,Labels)\n",
    "n.train(batches=1000,lr=0.2,epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([315, 330, 308, 334, 327, 285, 278, 320, 215, 288], dtype=int64),\n",
       " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288], dtype=int64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 88.8 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question3\n",
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class SingleLayerNeuralNetwork():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        #implementation of various activation functions and their derivatives\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                \n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "\n",
    "        return np.argmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0007244222394535747\n",
      "1 0.0003677269832465923\n",
      "2 0.00016489818961556115\n",
      "3 0.0001273821077311397\n",
      "4 0.00013661902539157614\n",
      "5 0.000145652677640878\n",
      "6 0.0001310232131454275\n",
      "7 9.886985731332751e-05\n",
      "8 6.824943936169827e-05\n",
      "9 5.177598870312134e-05\n",
      "10 4.343820046921563e-05\n",
      "11 3.853947512926637e-05\n",
      "12 3.5040965804228994e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n=SingleLayerNeuralNetwork(X_train,Labels)\n",
    "l1=Layer(100)\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4\n",
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class DoubleLayerNeuralNetwork():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        #implementation of various activation functions and their derivatives\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                \n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=DoubleLayerNeuralNetwork(X_train,Labels)\n",
    "l1=Layer(100)\n",
    "l2=Layer(100)\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question5\n",
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class NeuralNetworkActivations():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        #implementation of various activation functions and their derivatives\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        elif name=='relu':\n",
    "            if derivative==False:\n",
    "                return np.maximum(0.0,z)\n",
    "            else:\n",
    "              z[z<=0] = 0.0\n",
    "              z[z>0] = 1.0\n",
    "              return z\n",
    "        elif name=='tanh':\n",
    "          if derivative==False:\n",
    "                return np.tanh(z)\n",
    "          else:\n",
    "                return 1.0 - (np.tanh(z)) ** 2\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=NeuralNetworkActivations(X_train,Labels)\n",
    "l1=Layer(100,'sigmoid')\n",
    "l2=Layer(50, 'tanh')\n",
    "n.connect(X_train,l1)\n",
    "n.connect(l1,l2)\n",
    "n.connect(l2,Labels)\n",
    "n.train(batches=1000,lr=0.1,epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=n.predict(X_test)\n",
    "np.bincount(n.predict(X_test)),np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question6\n",
    "class Layer():\n",
    "    \"\"\"\n",
    "    size: Number of nodes in the hidden layer \n",
    "    activation: name of activation function for the layer\n",
    "    \"\"\"\n",
    "    def __init__(self,size,activation='sigmoid'): \n",
    "        self.shape=(1,size)\n",
    "        self.activation=activation\n",
    "                \n",
    "class NeuralNetworkMomentum():\n",
    "    def __init__(self,x,y):\n",
    "        \"\"\"\n",
    "        x is 2d array of input images\n",
    "        y are one hot encoded labels \n",
    "        \"\"\"\n",
    "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
    "        self.y=y\n",
    "        self.weights=[]\n",
    "        self.bias=[]\n",
    "        self.outputs=[]\n",
    "        self.derivatives=[]\n",
    "        self.activations=[]\n",
    "        self.delta_weights=[]\n",
    "        self.delta_bias=[]\n",
    "        \n",
    "    def connect(self,layer1,layer2):\n",
    "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
    "        #Initialise weights,derivatives and activation lists\n",
    "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.delta_weights.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
    "        self.delta_bias.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
    "        if isinstance(layer2,Layer):\n",
    "            self.activations.append(layer2.activation)\n",
    "            \n",
    "    def activation(self,name,z,derivative=False):\n",
    "        \n",
    "        #implementation of various activation functions and their derivatives\n",
    "        if name=='sigmoid':\n",
    "            if derivative==False:\n",
    "                return 1/(1+np.exp(-z))\n",
    "            else:\n",
    "                return z*(1-z)\n",
    "        elif name=='relu':\n",
    "            if derivative==False:\n",
    "                return np.maximum(0.0,z)\n",
    "            else:\n",
    "              z[z<=0] = 0.0\n",
    "              z[z>0] = 1.0\n",
    "              return z\n",
    "        elif name=='tanh':\n",
    "          if derivative==False:\n",
    "                return np.tanh(z)\n",
    "          else:\n",
    "                return 1.0 - (np.tanh(z)) ** 2\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        e=np.exp(z)\n",
    "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
    "    \n",
    "    def max_log_likelihood(self,y_pred,y):\n",
    "        \"\"\"cross entropy\"\"\"\n",
    "        return y*np.log(y_pred)\n",
    "    \n",
    "    def delta_mll(self,y,y_pred):\n",
    "        \"\"\"derivative of cross entropy\"\"\"\n",
    "        #return y*(y_pred-1)\n",
    "        return y_pred-y\n",
    "    \n",
    "    def forward_pass(self,x,y,weights,bias):\n",
    "        cost=0\n",
    "        self.outputs=[]\n",
    "        for i in range(len(weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            self.outputs.append(x) #append without adding ones array\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
    "            if i==len(weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        self.outputs.append(x)\n",
    "        self.y_pred=x\n",
    "        \n",
    "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
    "        cost=np.mean(np.sum(temp,axis=1))\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def backward_pass(self,y,lr,momentum=False,beta=0.9):\n",
    "        for i in range(len(self.weights)-1,-1,-1):\n",
    "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
    "            if i==len(self.weights)-1:\n",
    "                prev_term=self.delta_mll(y,self.y_pred)  \n",
    "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
    "            else:\n",
    "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
    "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
    "            if momentum:\n",
    "                self.delta_weights[i]=beta*self.delta_weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                self.delta_bias[i]=beta*self.delta_bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                self.weights[i]=self.weights[i]+self.delta_weights[i]\n",
    "                self.bias[i]=self.bias[i]+self.delta_bias[i]\n",
    "            else:\n",
    "                self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "                self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
    "    \n",
    "    def train(self,batches,lr=1e-3,epoch=10,beta):\n",
    "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
    "        for epochs in range(epoch):\n",
    "            samples=len(self.x)\n",
    "            c=0\n",
    "            for i in range(batches):\n",
    "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
    "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
    "              \n",
    "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
    "              self.backward_pass(y_batch,lr,momentum=True,beta)\n",
    "            print(epochs,c/batches)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"input: x_test values\"\"\"\n",
    "        x=x/255\n",
    "        for i in range(len(self.weights)):\n",
    "            samples=len(x)\n",
    "            ones_array=np.ones(samples).reshape(samples,1)\n",
    "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
    "            if i==len(self.weights)-1:\n",
    "                x=self.softmax(z)\n",
    "            else:\n",
    "                x=self.activation(self.activations[i],z)\n",
    "        return np.argmax(x,axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
